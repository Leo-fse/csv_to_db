This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.gitignore
.python-version
generate_dummy_data.py
main.py
pyproject.toml
README.md
src/config/config.py
src/db/db_utils.py
src/file/file_processor.py
src/file/file_utils.py
src/file/zip_handler.py
src/main.py
src/processor/csv_processor.py
tests/test_config_values.py
tests/test_env.py
tests/test_main.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".python-version">
3.11
</file>

<file path="generate_dummy_data.py">
#!/usr/bin/env python
"""
テスト用ダミーデータ生成スクリプト

センサーデータのCSVファイルを生成します。
"""

import argparse
import csv
import os
import random
import zipfile
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np


class DummyDataGenerator:
    """ダミーデータを生成するクラス"""

    # センサータイプのサンプル
    SENSOR_TYPES = [
        {"name": "温度", "unit": "℃", "min": 20, "max": 80, "pattern": "sine"},
        {"name": "圧力", "unit": "MPa", "min": 0.1, "max": 10, "pattern": "random"},
        {"name": "流量", "unit": "L/min", "min": 10, "max": 100, "pattern": "sine"},
        {"name": "振動", "unit": "mm/s", "min": 0, "max": 20, "pattern": "random"},
        {"name": "電流", "unit": "A", "min": 0.5, "max": 50, "pattern": "sine"},
        {"name": "電圧", "unit": "V", "min": 100, "max": 240, "pattern": "stable"},
        {"name": "回転数", "unit": "rpm", "min": 500, "max": 3000, "pattern": "sine"},
        {"name": "湿度", "unit": "%", "min": 30, "max": 90, "pattern": "random"},
        {"name": "pH", "unit": "", "min": 4, "max": 10, "pattern": "stable"},
        {
            "name": "導電率",
            "unit": "μS/cm",
            "min": 100,
            "max": 1000,
            "pattern": "random",
        },
    ]

    # 工場名のサンプル
    FACTORIES = ["AAA", "BBB", "CCC", "DDD", "EEE"]

    # 機器IDのサンプル
    MACHINE_IDS = ["No.1", "No.2", "No.3", "No.4", "No.5"]

    # データラベルのサンプル
    DATA_LABELS = ["２０２４年点検", "定期点検", "異常検知", "通常運転", "試験運転"]

    def __init__(
        self,
        output_dir="data",
        num_files=5,
        sensors_per_file=5,
        data_points=100,
        start_date=None,
        time_interval=60,
        file_prefix="test",
        create_zip=False,
    ):
        """
        初期化

        Parameters:
        output_dir (str): 出力ディレクトリ
        num_files (int): 生成するファイル数
        sensors_per_file (int): 各ファイルのセンサー数
        data_points (int): 各ファイルのデータポイント数
        start_date (datetime): 開始日時
        time_interval (int): 時間間隔（秒）
        file_prefix (str): ファイル名のプレフィックス
        create_zip (bool): ZIPファイルを作成するかどうか
        """
        self.output_dir = Path(output_dir)
        self.num_files = num_files
        self.sensors_per_file = sensors_per_file
        self.data_points = data_points
        self.start_date = start_date or datetime.now()
        self.time_interval = time_interval
        self.file_prefix = file_prefix
        self.create_zip = create_zip

    def generate_sensor_data(self, sensor_type, num_points):
        """
        センサーデータを生成する

        Parameters:
        sensor_type (dict): センサータイプ情報
        num_points (int): データポイント数

        Returns:
        list: 生成されたデータ値のリスト
        """
        min_val = sensor_type["min"]
        max_val = sensor_type["max"]
        pattern = sensor_type["pattern"]

        if pattern == "random":
            # ランダムな値
            return np.random.uniform(min_val, max_val, num_points).tolist()

        elif pattern == "sine":
            # サイン波パターン（ノイズ付き）
            x = np.linspace(0, 4 * np.pi, num_points)
            amplitude = (max_val - min_val) / 2
            offset = min_val + amplitude
            noise = np.random.normal(0, amplitude * 0.1, num_points)
            return (offset + amplitude * np.sin(x) + noise).tolist()

        elif pattern == "stable":
            # 安定した値（小さな変動あり）
            base_val = (min_val + max_val) / 2
            noise_level = (max_val - min_val) * 0.05
            return (base_val + np.random.normal(0, noise_level, num_points)).tolist()

        else:
            # デフォルトはランダム
            return np.random.uniform(min_val, max_val, num_points).tolist()

    def generate_csv_file(self, file_index):
        """
        CSVファイルを生成する

        Parameters:
        file_index (int): ファイルインデックス

        Returns:
        tuple: (ファイルパス, メタ情報)
        """
        # ファイル名を生成
        prefixes = ["Cond", "User", "test"]
        prefix = (
            random.choice(prefixes)
            if self.file_prefix == "random"
            else self.file_prefix
        )
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_name = f"{prefix}_{timestamp}_{file_index}.csv"
        file_path = self.output_dir / file_name

        # メタ情報を生成
        meta_info = {
            "factory": random.choice(self.FACTORIES),
            "machine_id": random.choice(self.MACHINE_IDS),
            "data_label": random.choice(self.DATA_LABELS),
        }

        # センサー情報を生成
        sensors = random.sample(
            self.SENSOR_TYPES, min(self.sensors_per_file, len(self.SENSOR_TYPES))
        )

        # 時間データを生成
        start_time = self.start_date
        timestamps = [
            (start_time + timedelta(seconds=i * self.time_interval)).strftime(
                "%Y/%m/%d %H:%M:%S"
            )
            for i in range(self.data_points)
        ]

        # センサーデータを生成
        sensor_data = []
        for sensor in sensors:
            data = self.generate_sensor_data(sensor, self.data_points)
            sensor_data.append(data)

        # CSVファイルを作成
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)

            # ヘッダー行1: センサーID
            sensor_ids = [f"S{i + 1:03d}" for i in range(len(sensors))]
            writer.writerow([""] + sensor_ids)

            # ヘッダー行2: センサー名
            writer.writerow([""] + [sensor["name"] for sensor in sensors])

            # ヘッダー行3: 単位
            writer.writerow([""] + [sensor["unit"] for sensor in sensors])

            # データ行
            for i in range(self.data_points):
                # 直接文字列を作成して書き込む
                line = timestamps[i]
                for j in range(len(sensors)):
                    line += f",{sensor_data[j][i]:.2f}"
                # 末尾にカンマを追加
                line += ","
                f.write(line + "\n")

        return file_path, meta_info

    def generate_all_files(self):
        """
        すべてのファイルを生成する

        Returns:
        list: 生成されたファイルのリスト
        """
        # 出力ディレクトリを作成
        self.output_dir.mkdir(parents=True, exist_ok=True)

        generated_files = []

        print(f"{self.num_files}個のダミーデータファイルを生成中...")

        for i in range(self.num_files):
            file_path, meta_info = self.generate_csv_file(i + 1)
            print(
                f"  生成: {file_path} (工場: {meta_info['factory']}, 機器: {meta_info['machine_id']})"
            )
            generated_files.append((file_path, meta_info))

        # ZIPファイルを作成する場合
        if self.create_zip and generated_files:
            zip_path = (
                self.output_dir
                / f"dummy_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
            )

            with zipfile.ZipFile(zip_path, "w") as zip_file:
                for file_path, _ in generated_files:
                    zip_file.write(file_path, file_path.name)

            print(f"ZIPファイルを作成: {zip_path}")

        return generated_files


def main():
    """メイン実行関数"""
    parser = argparse.ArgumentParser(description="テスト用ダミーデータ生成スクリプト")

    parser.add_argument(
        "--output-dir",
        type=str,
        default="data",
        help="出力ディレクトリ",
    )
    parser.add_argument(
        "--num-files",
        type=int,
        default=5,
        help="生成するファイル数",
    )
    parser.add_argument(
        "--sensors",
        type=int,
        default=5,
        help="各ファイルのセンサー数",
    )
    parser.add_argument(
        "--data-points",
        type=int,
        default=100,
        help="各ファイルのデータポイント数",
    )
    parser.add_argument(
        "--time-interval",
        type=int,
        default=60,
        help="時間間隔（秒）",
    )
    parser.add_argument(
        "--file-prefix",
        type=str,
        default="random",
        choices=["Cond", "User", "test", "random"],
        help="ファイル名のプレフィックス",
    )
    parser.add_argument(
        "--create-zip",
        action="store_true",
        help="生成したファイルをZIPにまとめる",
    )

    args = parser.parse_args()

    # ダミーデータ生成器を作成
    generator = DummyDataGenerator(
        output_dir=args.output_dir,
        num_files=args.num_files,
        sensors_per_file=args.sensors,
        data_points=args.data_points,
        time_interval=args.time_interval,
        file_prefix=args.file_prefix,
        create_zip=args.create_zip,
    )

    # ファイルを生成
    generator.generate_all_files()

    print("\n生成完了！")
    print(f"出力ディレクトリ: {args.output_dir}")


if __name__ == "__main__":
    main()
</file>

<file path="src/config/config.py">
"""
設定管理モジュール

環境変数の読み込みと設定の一元管理を行います。
"""

import os
from pathlib import Path

from dotenv import load_dotenv


class Config:
    """アプリケーション設定を管理するクラス"""

    _instance = None
    _initialized = False

    def __new__(cls):
        """シングルトンパターンの実装"""
        if cls._instance is None:
            cls._instance = super(Config, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        """設定の初期化"""
        # 二重初期化を防止
        if self._initialized:
            return
        self._initialized = True

        # 環境変数を読み込む
        load_dotenv()

        # 設定値を保持する辞書
        self._settings = {
            "folder": os.environ.get("folder", "data"),
            "pattern": os.environ.get("pattern", r"(Cond|User|test)"),
            "db": os.environ.get("db", "processed_files.duckdb"),
            "encoding": os.environ.get("encoding", "shift-jis"),
            "factory": os.environ.get("factory", "AAA"),
            "machine_id": os.environ.get("machine_id", "No.1"),
            "data_label": os.environ.get("data_label", "２０２４年点検"),
        }

    def get(self, key, default=None):
        """
        設定値を取得する

        Parameters:
        key (str): 設定キー
        default: キーが存在しない場合のデフォルト値

        Returns:
        設定値
        """
        return self._settings.get(key, default)

    def set(self, key, value):
        """
        設定値を設定する

        Parameters:
        key (str): 設定キー
        value: 設定値
        """
        self._settings[key] = value

    def get_all(self):
        """
        すべての設定値を取得する

        Returns:
        dict: すべての設定値
        """
        return self._settings.copy()

    def get_meta_info(self):
        """
        メタ情報を取得する

        Returns:
        dict: メタ情報（工場名、号機ID、データラベル名）
        """
        return {
            "factory": self.get("factory"),
            "machine_id": self.get("machine_id"),
            "data_label": self.get("data_label"),
        }


# 設定のグローバルインスタンス
config = Config()
</file>

<file path="src/file/file_utils.py">
"""
ファイル操作ユーティリティモジュール

ファイル検索、ハッシュ計算などのファイル操作に関する機能を提供します。
"""

import hashlib
import mmap
import os
import re
from pathlib import Path


class FileFinder:
    """ファイル検索を行うクラス"""

    def __init__(self, pattern=None):
        """
        初期化

        Parameters:
        pattern (str, optional): 正規表現パターン
        """
        self.pattern = pattern
        self.regex = re.compile(pattern) if pattern else None

    def set_pattern(self, pattern):
        """
        検索パターンを設定する

        Parameters:
        pattern (str): 正規表現パターン
        """
        self.pattern = pattern
        self.regex = re.compile(pattern)

    def find_csv_files(self, folder_path):
        """
        フォルダ内から正規表現パターンに一致するCSVファイルを検索する

        Parameters:
        folder_path (str or Path): 検索対象のフォルダパス

        Returns:
        list: [{'path': ファイルパス, 'source_zip': None}]
        """
        found_files = []

        # Pathオブジェクトへ変換
        folder = Path(folder_path)

        # 正規表現パターンが設定されていない場合はエラー
        if not self.regex:
            raise ValueError("検索パターンが設定されていません")

        # 通常のCSVファイルを検索
        for file in folder.rglob("*.csv"):
            if self.regex.search(file.name):
                found_files.append({"path": file, "source_zip": None})

        return found_files

    def find_files_with_extension(self, folder_path, extension):
        """
        指定した拡張子のファイルを検索する

        Parameters:
        folder_path (str or Path): 検索対象のフォルダパス
        extension (str): 拡張子（先頭のドットを含む、例: '.csv'）

        Returns:
        list: 見つかったファイルのPathオブジェクトのリスト
        """
        # Pathオブジェクトへ変換
        folder = Path(folder_path)

        # 拡張子の先頭のドットを確認
        if not extension.startswith("."):
            extension = "." + extension

        # ファイルを検索
        return list(folder.rglob(f"*{extension}"))


class FileHasher:
    """ファイルハッシュ計算を行うクラス"""

    @staticmethod
    def get_file_hash(file_path):
        """
        ファイルのSHA256ハッシュを計算する

        Parameters:
        file_path (str or Path): ハッシュを計算するファイルのパス

        Returns:
        str: SHA256ハッシュ値（16進数文字列）
        """
        sha256_hash = hashlib.sha256()

        # ファイルサイズを取得
        file_size = os.path.getsize(file_path)

        with open(file_path, "rb") as f:
            # 小さなファイルは通常の方法で処理
            if file_size < 1024 * 1024:  # 1MB未満
                sha256_hash.update(f.read())
            else:
                # 大きなファイルはメモリマッピングを使用
                try:
                    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                        # メモリマップされたファイルを直接ハッシュ計算に使用
                        sha256_hash.update(mm)
                except (ValueError, OSError):
                    # mmapが使用できない場合は通常の方法にフォールバック
                    f.seek(0)
                    for byte_block in iter(lambda: f.read(4096), b""):
                        sha256_hash.update(byte_block)

        return sha256_hash.hexdigest()
</file>

<file path="src/file/zip_handler.py">
"""
ZIPファイル処理モジュール

ZIPファイルからのファイル抽出などの機能を提供します。
"""

import zipfile
from pathlib import Path


class ZipHandler:
    """ZIPファイル処理を行うクラス"""

    @staticmethod
    def find_csv_files_in_zip(zip_path, pattern_regex):
        """
        ZIPファイル内から正規表現パターンに一致するCSVファイルを検索する

        Parameters:
        zip_path (str or Path): ZIPファイルのパス
        pattern_regex: コンパイル済み正規表現パターン

        Returns:
        list: [{'path': ファイルパス, 'source_zip': ZIPファイルパス}]
        """
        found_files = []

        try:
            with zipfile.ZipFile(zip_path, "r") as zip_ref:
                # ZIPファイル内のファイル一覧を取得
                zip_contents = zip_ref.namelist()

                # CSVファイルかつ条件に合うものを抽出
                for file_in_zip in zip_contents:
                    if file_in_zip.endswith(".csv") and pattern_regex.search(
                        Path(file_in_zip).name
                    ):
                        found_files.append(
                            {"path": file_in_zip, "source_zip": zip_path}
                        )
        except zipfile.BadZipFile:
            print(f"警告: {zip_path}は有効なZIPファイルではありません。")

        return found_files

    @staticmethod
    def extract_file(zip_path, file_path, output_dir):
        """
        ZIPファイルから特定のファイルを抽出する

        Parameters:
        zip_path (str or Path): ZIPファイルのパス
        file_path (str): 抽出するファイルのZIP内パス
        output_dir (str or Path): 出力先ディレクトリ

        Returns:
        Path: 抽出されたファイルのパス

        Raises:
        FileNotFoundError: ファイルが見つからない場合
        zipfile.BadZipFile: 無効なZIPファイルの場合
        """
        # 出力ディレクトリの確認と作成
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # ZIPファイルを開いて処理
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            # ZIPファイル内のファイルパスを正規化
            normalized_path = file_path.replace("\\", "/")

            # ファイル名のみを取得
            file_name = Path(normalized_path).name

            # 出力先のフルパス
            output_path = output_dir / file_name

            # ファイルを抽出
            try:
                # まずそのままのパスで試す
                zip_ref.extract(normalized_path, output_dir)
                # 階層構造があればそのファイルへのフルパスを返す
                if "/" in normalized_path:
                    return output_dir / normalized_path
                return output_path
            except KeyError:
                # 正確なパスでなければ、ファイル名でマッチするものを探す
                for zip_info in zip_ref.infolist():
                    zip_file_path = zip_info.filename.replace("\\", "/")
                    if (
                        zip_file_path.endswith("/" + file_name)
                        or zip_file_path == file_name
                    ):
                        # 見つかったファイルを抽出
                        zip_ref.extract(zip_info, output_dir)
                        # 抽出されたファイルのパスを返す
                        if "/" in zip_info.filename:
                            return output_dir / zip_info.filename
                        return output_dir / file_name

                # ファイルが見つからない場合はエラー
                raise FileNotFoundError(
                    f"ZIPファイル内に {file_path} または {file_name} が見つかりません。"
                )
</file>

<file path="src/processor/csv_processor.py">
"""
CSV処理モジュール

CSVファイルの読み込み、データ変換などの機能を提供します。
"""

import codecs
import os
import tempfile
from pathlib import Path

import polars as pl

from src.config.config import config


class CsvProcessor:
    """CSVファイル処理を行うクラス"""

    def __init__(self, encoding=None):
        """
        初期化

        Parameters:
        encoding (str, optional): CSVファイルのエンコーディング
        """
        self.encoding = encoding or config.get("encoding", "shift-jis")

    def process_csv_file(self, file_path):
        """
        CSVファイルを処理する

        Parameters:
        file_path (str or Path): 処理するCSVファイルのパス

        Returns:
        pl.DataFrame: 処理されたデータフレーム
        """
        print(f"処理中: {file_path}")

        # ファイル全体を一度に読み込む
        encoding = self.encoding

        # 一時ファイルのパス（初期値はNone）
        temp_path = None

        # Shift-JISエンコーディングの場合、一度ファイルを読み込んでUTF-8に変換
        if encoding.lower() in ["shift-jis", "shift_jis", "sjis", "cp932", "ms932"]:
            # 一時ファイルを作成
            with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as temp_file:
                temp_path = temp_file.name

            try:
                # テキストモードでの読み込みを試みる
                with open(file_path, "r", encoding=encoding) as src_file:
                    content = src_file.read()
                    with open(temp_path, "w", encoding="utf-8") as dest_file:
                        dest_file.write(content)
            except UnicodeDecodeError:
                # エンコーディングエラーが発生した場合、バイナリモードで読み込み
                with open(file_path, "rb") as src_file:
                    content = src_file.read()
                    # バイナリデータをShift-JISとしてデコードし、UTF-8にエンコード
                    try:
                        decoded = content.decode(encoding, errors="replace")
                        with open(temp_path, "w", encoding="utf-8") as dest_file:
                            dest_file.write(decoded)
                    except Exception as e:
                        print(f"エンコーディング変換エラー: {str(e)}")
                        # 最終手段：バイナリデータをそのまま書き込み、Polarsのutf8-lossyで処理
                        with open(temp_path, "wb") as dest_file:
                            dest_file.write(content)
                        # 以降の処理ではutf8-lossyを使用
                        encoding = "utf8-lossy"

            # 一時ファイルを処理対象に変更
            file_path = temp_path
            # 以降の処理ではUTF-8として扱う
            encoding = "utf-8"

        try:
            # Polarsのscan_csvは'utf8'または'utf8-lossy'のみをサポート
            polars_encoding = (
                "utf8" if encoding.lower() in ["utf-8", "utf8"] else "utf8-lossy"
            )

            # LazyFrameとDataFrameの変数
            lazy_df = None
            header_df = None
            data_df = None

            lazy_df = pl.scan_csv(
                file_path,
                has_header=False,
                truncate_ragged_lines=True,
                encoding=polars_encoding,
                infer_schema_length=10000,  # スキーマ推論の範囲を増やす
            )

            # ヘッダー部分（最初の3行）を取得
            header_df = lazy_df.slice(0, 3).collect()

            # データ部分（4行目以降）を取得し、最後の列（空白列）を除外
            data_df = lazy_df.slice(3, None).collect()[:, :-1]

            # 列名を設定する（変換前）
            column_names = ["Time"] + [f"col_{i}" for i in range(1, data_df.width)]
            data_df.columns = column_names

            # 縦持ちデータにしたい
            # １列目を日時として、残りの列を値として読み込む
            data_df = data_df.unpivot(
                index=["Time"],
                on=[f"col_{i}" for i in range(1, data_df.width)],
                variable_name="sensor_column",
                value_name="value",
            )

            # センサー情報のマッピングを作成
            sensor_ids = list(header_df.row(0)[1:])
            sensor_names = list(header_df.row(1)[1:])
            sensor_units = list(header_df.row(2)[1:])

            # センサー情報のDataFrameを作成（ベクトル化処理のため）
            sensor_df = pl.DataFrame(
                {
                    "sensor_column": [f"col_{i + 1}" for i in range(len(sensor_ids))],
                    "sensor_id": sensor_ids,
                    "sensor_name": sensor_names,
                    "unit": sensor_units,
                }
            )

            # 結合操作でセンサー情報を追加（ベクトル化された処理）
            data_df = data_df.join(sensor_df, on="sensor_column", how="left")

            # Filter out rows where both sensor_id and sensor_name are "-"
            data_df = data_df.filter(
                ~(
                    (pl.col("sensor_name").str.strip_chars() == "-")
                    & (pl.col("unit").str.strip_chars() == "-")
                )
            )

            # Time列の末尾の空白を除去し、datetime型に変換する
            data_df = data_df.with_columns(
                pl.col("Time")
                .str.strip_chars()
                .str.strptime(pl.Datetime, format="%Y/%m/%d %H:%M:%S")
            )

            # Remove the sensor_column from the results
            data_df = data_df.drop("sensor_column")
            # Remove duplicate rows based on all columns
            data_df = data_df.unique()

            return data_df

        finally:
            # 一時ファイルを削除（Shift-JISからの変換時のみ）
            if temp_path and os.path.exists(temp_path):
                try:
                    os.unlink(temp_path)
                except:
                    pass

    def add_meta_info(self, data_df, file_info, meta_info=None):
        """
        データフレームにメタ情報を追加する

        Parameters:
        data_df (pl.DataFrame): 処理されたデータフレーム
        file_info (dict): ファイル情報
        meta_info (dict, optional): メタ情報

        Returns:
        pl.DataFrame: メタ情報が追加されたデータフレーム
        """
        if meta_info is None:
            meta_info = config.get_meta_info()

        # ソースファイル情報とメタ情報を列として追加
        return data_df.with_columns(
            [
                pl.lit(str(file_info["file_path"])).alias("source_file"),
                pl.lit(
                    str(file_info["source_zip"]) if file_info["source_zip"] else ""
                ).alias("source_zip"),
                pl.lit(meta_info.get("factory", "")).alias("factory"),
                pl.lit(meta_info.get("machine_id", "")).alias("machine_id"),
                pl.lit(meta_info.get("data_label", "")).alias("data_label"),
            ]
        )
</file>

<file path="tests/test_config_values.py">
"""
設定値確認スクリプト

コマンドライン引数とconfig.pyから読み込まれた設定値を表示します。
"""

import argparse

from src.config.config import config


def main():
    """メイン実行関数"""
    # コマンドライン引数の解析
    parser = argparse.ArgumentParser(description="設定値確認ツール")
    parser.add_argument(
        "--folder",
        type=str,
        default=config.get("folder"),
        help="検索対象のフォルダパス",
    )
    parser.add_argument(
        "--pattern",
        type=str,
        default=config.get("pattern"),
        help="ファイル名フィルタリングのための正規表現パターン",
    )
    parser.add_argument(
        "--db",
        type=str,
        default=config.get("db"),
        help="処理記録用データベースファイルのパス",
    )
    parser.add_argument(
        "--factory",
        type=str,
        default=config.get("factory"),
        help="工場名",
    )
    parser.add_argument(
        "--machine-id",
        type=str,
        default=config.get("machine_id"),
        help="号機ID",
    )
    parser.add_argument(
        "--data-label",
        type=str,
        default=config.get("data_label"),
        help="データラベル名",
    )

    args = parser.parse_args()

    # configから直接取得した値
    print("===== configから直接取得した値 =====")
    config_values = config.get_all()
    for key, value in config_values.items():
        print(f"{key}: {value}")

    # コマンドライン引数から取得した値（デフォルト値を含む）
    print("\n===== コマンドライン引数から取得した値 =====")
    print(f"folder: {args.folder}")
    print(f"pattern: {args.pattern}")
    print(f"db: {args.db}")
    print(f"factory: {args.factory}")
    print(f"machine_id: {args.machine_id}")
    print(f"data_label: {args.data_label}")

    # メタ情報
    print("\n===== メタ情報 =====")
    meta_info = config.get_meta_info()
    for key, value in meta_info.items():
        print(f"{key}: {value}")


if __name__ == "__main__":
    main()
</file>

<file path="tests/test_env.py">
"""
環境変数読み込みテスト

.envファイルから環境変数が正しく読み込まれているかをテストします。
"""

from src.config.config import config


def test_env_variables():
    """環境変数が正しく読み込まれているかをテストする関数"""
    print("===== 環境変数読み込みテスト =====")

    # .envファイルの期待値
    expected = {
        "folder": "data",
        "pattern": "(Cond|User|test)",
        "db": "sensor_data.duckdb",
        "encoding": "utf-8",
        "factory": "AAA",
        "machine_id": "No.1",
        "data_label": "２０２４年点検",
    }

    # 実際に読み込まれた値
    actual = config.get_all()

    print("\n実際に読み込まれた値:")
    for key, value in actual.items():
        print(f"{key}: {value}")

    print("\n期待値との比較:")
    all_correct = True
    for key, expected_value in expected.items():
        actual_value = actual.get(key)
        is_correct = actual_value == expected_value
        status = "✓" if is_correct else "✗"
        print(f"{status} {key}: 期待値={expected_value}, 実際値={actual_value}")
        if not is_correct:
            all_correct = False

    print("\n結果:", "すべて正常" if all_correct else "不一致あり")

    return all_correct


if __name__ == "__main__":
    test_env_variables()
</file>

<file path="tests/test_main.py">
"""
テストモジュール

リファクタリングされたコードのテストを行います。
"""

import os
import tempfile
import unittest
from pathlib import Path

import duckdb
import polars as pl

from src.config.config import config
from src.db.db_utils import DatabaseManager
from src.file.file_processor import FileProcessor
from src.file.file_utils import FileFinder, FileHasher
from src.file.zip_handler import ZipHandler
from src.processor.csv_processor import CsvProcessor


class TestCSVProcessing(unittest.TestCase):
    """CSVファイル処理機能のテストケース"""

    def setUp(self):
        """テスト前の準備"""
        # 一時ディレクトリを作成
        self.temp_dir = tempfile.TemporaryDirectory()
        self.temp_path = Path(self.temp_dir.name)

        # テスト用CSVファイルを作成
        self.test_csv_path = self.temp_path / "test.csv"
        with open(self.test_csv_path, "w", encoding="utf-8") as f:
            f.write(", 1000, 1001, 1000, 1002\n")
            f.write(", param_A, param_B, param_A, param_C\n")
            f.write(", kg, mm, kg, cm\n")
            f.write('2024/1/1 00:00:00,1,2,"a",4,\n')
            f.write('2024/1/1 00:00:01,1,2,"a",4,\n')

        # テスト用データベースを作成
        self.db_path = self.temp_path / "test.duckdb"
        self.db_manager = DatabaseManager(self.db_path)

    def tearDown(self):
        """テスト後のクリーンアップ"""
        # データベース接続を閉じる
        self.db_manager.close()
        # 一時ディレクトリを削除
        self.temp_dir.cleanup()

    def test_find_csv_files(self):
        """find_csv_files関数のテスト"""
        # テスト用のCSVファイルを追加
        (self.temp_path / "test2.csv").touch()
        (self.temp_path / "other.txt").touch()

        # ファイル検索オブジェクトを作成
        file_finder = FileFinder(r"test")

        # 関数を実行
        files = file_finder.find_csv_files(self.temp_path)

        # 結果を検証
        self.assertEqual(len(files), 2)
        self.assertIn(str(self.test_csv_path), [str(f["path"]) for f in files])

    def test_get_file_hash(self):
        """get_file_hash関数のテスト"""
        # 関数を実行
        hash1 = FileHasher.get_file_hash(self.test_csv_path)

        # 同じファイルのハッシュは同じになることを確認
        hash2 = FileHasher.get_file_hash(self.test_csv_path)
        self.assertEqual(hash1, hash2)

        # 内容が異なるファイルのハッシュは異なることを確認
        different_file = self.temp_path / "different.csv"
        with open(different_file, "w", encoding="utf-8") as f:
            f.write("different content")
        hash3 = FileHasher.get_file_hash(different_file)
        self.assertNotEqual(hash1, hash3)

    def test_process_csv_file(self):
        """process_csv_file関数のテスト"""
        # CSVプロセッサを作成
        csv_processor = CsvProcessor(encoding="utf-8")

        # 関数を実行
        result_df = csv_processor.process_csv_file(self.test_csv_path)

        # 結果を検証
        self.assertIsInstance(result_df, pl.DataFrame)
        # 行数は処理結果に依存するため、厳密な値ではなく存在確認のみ行う
        self.assertGreater(result_df.height, 0)
        self.assertIn("Time", result_df.columns)
        self.assertIn("value", result_df.columns)
        self.assertIn("sensor_id", result_df.columns)
        self.assertIn("sensor_name", result_df.columns)
        self.assertIn("unit", result_df.columns)

    def test_file_processor(self):
        """FileProcessorクラスのテスト"""
        # テスト用のCSVファイルリストを作成
        csv_files = [{"path": self.test_csv_path, "source_zip": None}]

        # ファイルプロセッサを作成
        file_processor = FileProcessor(self.db_path)

        # ファイルハッシュを計算
        file_hash = FileHasher.get_file_hash(self.test_csv_path)

        # テスト用のファイル情報を作成
        file_info = {
            "file_path": self.test_csv_path,
            "actual_file_path": self.test_csv_path,
            "source_zip": None,
            "source_zip_str": None,
            "file_hash": file_hash,
        }

        # 一時ディレクトリを作成
        with tempfile.TemporaryDirectory() as temp_dir:
            # 単一ファイル処理関数を直接呼び出し
            result = file_processor.process_single_file(file_info, Path(temp_dir))

            # 結果を検証
            self.assertTrue(result["success"])

        # データベースにデータが挿入されていることを確認
        result = self.db_manager.execute("SELECT COUNT(*) FROM sensor_data").fetchone()
        self.assertGreater(result[0], 0)

        # 同じファイルを再度処理すると、スキップされることを確認
        stats = file_processor.process_csv_files(csv_files)
        self.assertEqual(stats["already_processed_by_path"], 1)


if __name__ == "__main__":
    unittest.main()
</file>

<file path=".gitignore">
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
data
</file>

<file path="src/db/db_utils.py">
"""
データベース操作モジュール

DuckDBデータベースの初期化、処理済みファイル管理などの機能を提供します。
"""

import datetime
from pathlib import Path

import duckdb


class DatabaseManager:
    """データベース操作を行うクラス"""

    def __init__(self, db_path):
        """
        初期化

        Parameters:
        db_path (str or Path): データベースファイルのパス
        """
        self.db_path = Path(db_path)
        self.conn = None
        self.read_only = False
        self.setup_database()

    def setup_database(self):
        """
        データベースを初期化する

        Returns:
        duckdb.DuckDBPyConnection: データベース接続
        """
        try:
            # 通常モードで接続を試みる
            self.conn = duckdb.connect(str(self.db_path))
            self.read_only = False
        except duckdb.IOException as e:
            if "File is already open" in str(e):
                print(
                    f"警告: データベースファイル {self.db_path} は既に別のプロセスで開かれています。"
                )
                print("読み取り専用モードで接続を試みます...")
                try:
                    # 読み取り専用モードで接続を試みる
                    self.conn = duckdb.connect(str(self.db_path), read_only=True)
                    self.read_only = True
                    print(
                        "読み取り専用モードで接続しました。データの変更はできません。"
                    )
                except Exception as e2:
                    print(f"エラー: 読み取り専用モードでの接続にも失敗しました: {e2}")
                    raise
            else:
                raise

        # processed_filesテーブルを作成し、file_hashに一意性制約を追加
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS processed_files (
                file_path VARCHAR NOT NULL,
                file_hash VARCHAR NOT NULL,
                source_zip VARCHAR,
                processed_date TIMESTAMP,
                PRIMARY KEY (file_path, source_zip)
            )
        """)

        # file_hashに一意性インデックスが存在するか確認
        result = self.conn.execute("""
            SELECT COUNT(*) 
            FROM duckdb_indexes() 
            WHERE table_name = 'processed_files' AND index_name = 'idx_processed_files_hash'
        """).fetchone()

        # インデックスが存在しない場合は作成
        if result[0] == 0:
            self.conn.execute("""
                CREATE UNIQUE INDEX IF NOT EXISTS idx_processed_files_hash 
                ON processed_files(file_hash)
            """)

        # センサーデータ格納テーブル
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS sensor_data (
                Time TIMESTAMP,
                value VARCHAR,
                sensor_id VARCHAR,
                sensor_name VARCHAR,
                unit VARCHAR,
                source_file VARCHAR,
                source_zip VARCHAR,
                factory VARCHAR,
                machine_id VARCHAR,
                data_label VARCHAR
            )
        """)

        return self.conn

    def close(self):
        """データベース接続を閉じる"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def is_file_processed_by_path(self, file_path, source_zip=None):
        """
        ファイルパスに基づいて処理済みかどうかを確認する

        Parameters:
        file_path (str or Path): ファイルパス
        source_zip (str or Path, optional): ZIPファイルパス

        Returns:
        bool: 処理済みの場合はTrue
        """
        source_zip_value = "" if source_zip is None else str(source_zip)
        result = self.conn.execute(
            "SELECT COUNT(*) FROM processed_files WHERE file_path = ? AND source_zip = ?",
            [str(file_path), source_zip_value],
        ).fetchone()

        return result[0] > 0

    def is_file_processed_by_hash(self, file_hash):
        """
        ファイルハッシュに基づいて処理済みかどうかを確認する

        Parameters:
        file_hash (str): ファイルハッシュ

        Returns:
        bool: 処理済みの場合はTrue
        """
        result = self.conn.execute(
            "SELECT COUNT(*) FROM processed_files WHERE file_hash = ?", [file_hash]
        ).fetchone()

        return result[0] > 0

    def mark_file_as_processed(self, file_path, file_hash, source_zip=None):
        """
        ファイルを処理済みとしてデータベースに記録する

        Parameters:
        file_path (str or Path): ファイルパス
        file_hash (str): ファイルハッシュ
        source_zip (str or Path, optional): ZIPファイルパス

        Returns:
        bool: 成功した場合はTrue
        """
        # 読み取り専用モードの場合は何もせずにTrueを返す
        if self.read_only:
            print(
                f"  情報: 読み取り専用モードのため、処理済み記録はスキップします: {file_path}"
            )
            return True

        now = datetime.datetime.now()
        source_zip_value = "" if source_zip is None else str(source_zip)

        # UPSERTパターンを使用して挿入（一意制約違反を防ぐ）
        try:
            self.conn.execute(
                """
                INSERT INTO processed_files (file_path, file_hash, source_zip, processed_date) 
                VALUES (?, ?, ?, ?)
            """,
                [str(file_path), file_hash, source_zip_value, now],
            )
            return True
        except duckdb.ConstraintException:
            print(f"  情報: 同一ハッシュ({file_hash})のファイルが既に処理済みです")
            return False

    def unmark_file_as_processed(self, file_path, source_zip=None):
        """
        ファイルの処理済みマークをデータベースから削除する

        Parameters:
        file_path (str or Path): ファイルパス
        source_zip (str or Path, optional): ZIPファイルパス

        Returns:
        bool: 成功した場合はTrue
        """
        # 読み取り専用モードの場合は何もせずにTrueを返す
        if self.read_only:
            print(
                f"  情報: 読み取り専用モードのため、処理済み記録の削除はスキップします: {file_path}"
            )
            return True

        source_zip_value = "" if source_zip is None else str(source_zip)

        try:
            # ファイルパスとソースZIPに基づいて処理済み記録を削除
            self.conn.execute(
                """
                DELETE FROM processed_files 
                WHERE file_path = ? AND source_zip = ?
            """,
                [str(file_path), source_zip_value],
            )
            print(f"  情報: {file_path} の処理済みマークを解除しました")
            return True
        except Exception as e:
            print(f"  警告: 処理済みマーク解除中にエラー ({file_path}): {str(e)}")
            return False

    def insert_sensor_data(self, data_df):
        """
        センサーデータをデータベースに挿入する

        Parameters:
        data_df: Polars DataFrame

        Returns:
        int: 挿入された行数
        """
        # 読み取り専用モードの場合は何もせずに行数を返す
        if self.read_only:
            print(
                "  情報: 読み取り専用モードのため、センサーデータの挿入はスキップします"
            )
            return len(data_df) if data_df is not None else 0

        # DataFrameをArrowテーブルに変換
        arrow_table = data_df.to_arrow()

        # 一時テーブルとして登録
        self.conn.register("temp_sensor_data", arrow_table)

        # トランザクションを開始
        self.conn.execute("BEGIN TRANSACTION")

        try:
            # SQLで一括挿入（Arrow形式からの直接挿入）
            result = self.conn.execute("""
                INSERT INTO sensor_data 
                SELECT * FROM temp_sensor_data
            """)

            # 一時テーブルを削除
            self.conn.execute("DROP VIEW IF EXISTS temp_sensor_data")

            # 挿入された行数を取得（DuckDBでは直接取得できないため、データフレームの行数を使用）
            row_count = len(data_df) if data_df is not None else 0

            return row_count
        except Exception as e:
            # エラーが発生した場合はロールバック
            self.conn.execute("ROLLBACK")
            raise e

    def commit(self):
        """変更をコミットする"""
        if self.conn and not self.read_only:
            try:
                self.conn.execute("COMMIT")
            except duckdb.duckdb.TransactionException:
                # トランザクションがアクティブでない場合は無視
                pass

    def rollback(self):
        """変更をロールバックする"""
        if self.conn and not self.read_only:
            try:
                self.conn.execute("ROLLBACK")
            except duckdb.duckdb.TransactionException:
                # トランザクションがアクティブでない場合は無視
                pass

    def execute(self, query, params=None):
        """
        SQLクエリを実行する

        Parameters:
        query (str): SQLクエリ
        params (list, optional): クエリパラメータ

        Returns:
        duckdb.DuckDBPyResult: クエリ結果
        """
        if params:
            return self.conn.execute(query, params)
        return self.conn.execute(query)
</file>

<file path="src/main.py">
"""
CSVファイル処理ツール

センサーデータCSVファイルを検索、処理し、DuckDBデータベースに保存します。
"""

import argparse
import concurrent.futures

from src.config.config import config
from src.file.file_processor import FileProcessor


def main():
    """メイン実行関数"""
    try:
        # コマンドライン引数の解析
        parser = argparse.ArgumentParser(description="CSVファイル処理ツール")
        parser.add_argument(
            "--folder",
            type=str,
            default=config.get("folder"),
            help="検索対象のフォルダパス",
        )
        parser.add_argument(
            "--pattern",
            type=str,
            default=config.get("pattern"),
            help="ファイル名フィルタリングのための正規表現パターン",
        )
        parser.add_argument(
            "--db",
            type=str,
            default=config.get("db"),
            help="処理記録用データベースファイルのパス",
        )
        parser.add_argument(
            "--process-all",
            action="store_true",
            help="処理済みファイルも再処理する場合に指定",
        )
        parser.add_argument(
            "--factory",
            type=str,
            default=config.get("factory"),
            help="工場名",
        )
        parser.add_argument(
            "--machine-id",
            type=str,
            default=config.get("machine_id"),
            help="号機ID",
        )
        parser.add_argument(
            "--data-label",
            type=str,
            default=config.get("data_label"),
            help="データラベル名",
        )

        args = parser.parse_args()

        # メタ情報の辞書を作成
        meta_info = {
            "factory": args.factory,
            "machine_id": args.machine_id,
            "data_label": args.data_label,
        }

        # ファイル処理オブジェクトを作成
        processor = FileProcessor(args.db, meta_info)

        # フォルダ内のCSVファイルを処理
        stats = processor.process_folder(args.folder, args.pattern, args.process_all)

        # 結果の表示
        print("\n---- 処理結果 ----")
        print(f"見つかったファイル数: {stats['total_found']}")
        print(f"パスで既に処理済み: {stats['already_processed_by_path']}")
        print(f"内容が同一で処理済み: {stats['already_processed_by_hash']}")
        print(f"新たに処理: {stats['newly_processed']}")
        print(f"処理失敗: {stats['failed']}")
        # タイムアウトによる処理失敗件数を表示
        if "timeout" in stats:
            print(f"タイムアウト: {stats['timeout']}")
    except concurrent.futures.TimeoutError as e:
        print(f"\nエラー: 処理がタイムアウトしました: {str(e)}")
        if "stats" in locals():
            # タイムアウトが発生しても統計情報を表示
            print("\n---- 処理結果（タイムアウト発生） ----")
            print(f"見つかったファイル数: {stats['total_found']}")
            print(f"パスで既に処理済み: {stats['already_processed_by_path']}")
            print(f"内容が同一で処理済み: {stats['already_processed_by_hash']}")
            print(f"新たに処理: {stats['newly_processed']}")
            print(f"処理失敗: {stats['failed']}")
            if "timeout" in stats:
                print(f"タイムアウト: {stats['timeout']}")
    except Exception as e:
        print(f"\nエラー: 処理中に例外が発生しました: {str(e)}")


# 使用例
if __name__ == "__main__":
    main()
</file>

<file path="src/file/file_processor.py">
"""
ファイル処理モジュール

CSVファイルの検索、抽出、処理を統合的に行います。
"""

import concurrent.futures
import os
import re
import shutil
import tempfile
import threading
from pathlib import Path

from src.config.config import config
from src.db.db_utils import DatabaseManager
from src.file.file_utils import FileFinder, FileHasher
from src.file.zip_handler import ZipHandler
from src.processor.csv_processor import CsvProcessor


class FileProcessor:
    """ファイル処理を行うクラス"""

    def __init__(self, db_path=None, meta_info=None):
        """
        初期化

        Parameters:
        db_path (str or Path, optional): データベースファイルのパス
        meta_info (dict, optional): メタ情報
        """
        self.db_path = db_path or config.get("db")
        self.meta_info = meta_info or config.get_meta_info()
        self.db_manager = DatabaseManager(self.db_path)
        self.csv_processor = CsvProcessor()

        # ファイルロックを管理するための辞書
        self.file_locks = {}
        self.lock_dict_lock = threading.Lock()

    def __del__(self):
        """デストラクタ"""
        if hasattr(self, "db_manager"):
            self.db_manager.close()

    def get_file_lock(self, file_path):
        """
        ファイルロックを取得する

        Parameters:
        file_path (str): ファイルパス

        Returns:
        threading.Lock: ファイルロック
        """
        with self.lock_dict_lock:
            if file_path not in self.file_locks:
                self.file_locks[file_path] = threading.Lock()
            return self.file_locks[file_path]

    def find_csv_files(self, folder_path, pattern):
        """
        フォルダ内およびZIPファイル内から正規表現パターンに一致するCSVファイルを抽出する

        Parameters:
        folder_path (str or Path): 検索対象のフォルダパス
        pattern (str): 正規表現パターン

        Returns:
        list: [{'path': ファイルパス, 'source_zip': ZIPファイルパス（ない場合はNone）}]
        """
        # コンパイル済み正規表現パターン
        regex = re.compile(pattern)

        # ファイル検索オブジェクト
        file_finder = FileFinder(pattern)

        # 通常のCSVファイルを検索
        found_files = file_finder.find_csv_files(folder_path)

        # ZIPファイルを検索して中身を確認
        for zip_file in Path(folder_path).rglob("*.zip"):
            zip_files = ZipHandler.find_csv_files_in_zip(zip_file, regex)
            found_files.extend(zip_files)

        return found_files

    def process_single_file(self, file_info, temp_dir):
        """
        単一のCSVファイルを処理する関数

        Parameters:
        file_info (dict): 処理するファイルの情報
        temp_dir (Path): 一時ディレクトリのパス

        Returns:
        dict: 処理結果
        """
        result = {
            "success": False,
            "file_path": file_info["file_path"],
            "source_zip": file_info["source_zip"],
            "file_hash": file_info["file_hash"],
        }

        try:
            # ファイルを処理
            data_df = self.csv_processor.process_csv_file(file_info["actual_file_path"])
            if data_df is not None:
                # メタ情報を追加
                data_df = self.csv_processor.add_meta_info(
                    data_df, file_info, self.meta_info
                )

                # トランザクションを開始（insert_sensor_data内で開始されるため不要）
                # データベースに保存
                rows_inserted = self.db_manager.insert_sensor_data(data_df)

                # 処理済みに記録
                self.db_manager.mark_file_as_processed(
                    file_info["file_path"],
                    file_info["file_hash"],
                    file_info["source_zip_str"],
                )

                # コミット
                self.db_manager.commit()
                result["success"] = True
                result["rows_inserted"] = rows_inserted
            else:
                print(f"エラー: {file_info['file_path']} の処理結果がNoneです")
        except Exception as e:
            # ロールバック
            self.db_manager.rollback()

            print(
                f"エラー処理中 {file_info['file_path']}"
                + (
                    f" (in {file_info['source_zip']})"
                    if file_info["source_zip"]
                    else ""
                )
                + f": {str(e)}"
            )

        return result

    def process_csv_files(self, csv_files, process_all=False):
        """
        CSVファイルのリストを処理する

        Parameters:
        csv_files (list): 処理するCSVファイルのリスト
        process_all (bool): 処理済みファイルも再処理するかどうか

        Returns:
        dict: 処理結果の統計情報
        """
        # 結果統計
        stats = {
            "total_found": len(csv_files),
            "already_processed_by_path": 0,
            "already_processed_by_hash": 0,
            "newly_processed": 0,
            "failed": 0,
            "timeout": 0,  # タイムアウトによる失敗件数を追加
        }

        # 一時ディレクトリを作成
        temp_dir = Path(tempfile.mkdtemp())

        try:
            # 処理対象ファイルのリストを作成
            files_to_process = []

            # 前処理：処理済みファイルのフィルタリング
            for file_info in csv_files:
                file_path = file_info["path"]
                source_zip = file_info["source_zip"]
                source_zip_str = str(source_zip) if source_zip else None

                # パスベースで既に処理済みかチェック
                if not process_all and self.db_manager.is_file_processed_by_path(
                    file_path, source_zip_str
                ):
                    stats["already_processed_by_path"] += 1
                    print(
                        f"スキップ (既処理 - パス一致): {file_path}"
                        + (f" (in {source_zip})" if source_zip else "")
                    )
                    continue

                # ZIPファイル内のファイルなら抽出
                try:
                    if source_zip:
                        # 一時ファイルにZIPから抽出
                        actual_file_path = ZipHandler.extract_file(
                            source_zip, file_path, temp_dir
                        )
                    else:
                        actual_file_path = file_path

                    # ファイルハッシュを計算
                    file_hash = FileHasher.get_file_hash(actual_file_path)

                    # ハッシュベースで既に処理済みかチェック
                    if not process_all and self.db_manager.is_file_processed_by_hash(
                        file_hash
                    ):
                        stats["already_processed_by_hash"] += 1
                        print(
                            f"スキップ (既処理 - 内容一致): {file_path}"
                            + (f" (in {source_zip})" if source_zip else "")
                        )
                        continue

                    # 処理対象リストに追加
                    files_to_process.append(
                        {
                            "file_path": file_path,
                            "actual_file_path": actual_file_path,
                            "source_zip": source_zip,
                            "source_zip_str": source_zip_str,
                            "file_hash": file_hash,
                        }
                    )
                except Exception as e:
                    print(
                        f"エラー前処理中 {file_path}"
                        + (f" (in {source_zip})" if source_zip else "")
                        + f": {str(e)}"
                    )
                    stats["failed"] += 1

            # 並列処理の方法を選択
            # ファイル数が少ない場合は逐次処理、多い場合は並列処理
            if len(files_to_process) <= 1:
                # 逐次処理
                print("逐次処理を開始")
                for file_info in files_to_process:
                    result = self.process_single_file(file_info, temp_dir)
                    if result["success"]:
                        stats["newly_processed"] += 1
                    else:
                        stats["failed"] += 1
            else:
                # 並列処理（ThreadPoolExecutorを使用）
                max_workers = min(4, len(files_to_process))  # 最大4スレッド
                print(f"並列処理を開始: {max_workers}スレッド")

                # ファイルロックを使用して処理する関数
                def process_with_lock(file_info):
                    # ファイルロックを取得
                    file_path = str(file_info["actual_file_path"])
                    lock = self.get_file_lock(file_path)

                    # 各スレッド用の独立したデータベース接続を作成
                    thread_db_manager = DatabaseManager(self.db_path)

                    try:
                        # ロックを取得してファイルを処理
                        with lock:
                            # ファイルを処理
                            result = {
                                "success": False,
                                "file_path": file_info["file_path"],
                                "source_zip": file_info["source_zip"],
                                "file_hash": file_info["file_hash"],
                            }

                            try:
                                # ファイルを処理
                                data_df = self.csv_processor.process_csv_file(
                                    file_info["actual_file_path"]
                                )
                                if data_df is not None:
                                    # メタ情報を追加
                                    data_df = self.csv_processor.add_meta_info(
                                        data_df, file_info, self.meta_info
                                    )

                                    # データベースに保存
                                    rows_inserted = (
                                        thread_db_manager.insert_sensor_data(data_df)
                                    )

                                    # 処理済みに記録
                                    thread_db_manager.mark_file_as_processed(
                                        file_info["file_path"],
                                        file_info["file_hash"],
                                        file_info["source_zip_str"],
                                    )

                                    # コミット
                                    thread_db_manager.commit()
                                    result["success"] = True
                                    result["rows_inserted"] = rows_inserted
                                else:
                                    print(
                                        f"エラー: {file_info['file_path']} の処理結果がNoneです"
                                    )
                            except Exception as e:
                                # ロールバック
                                thread_db_manager.rollback()

                                print(
                                    f"エラー処理中 {file_info['file_path']}"
                                    + (
                                        f" (in {file_info['source_zip']})"
                                        if file_info["source_zip"]
                                        else ""
                                    )
                                    + f": {str(e)}"
                                )

                            return result
                    finally:
                        # データベース接続を閉じる
                        thread_db_manager.close()

                # タイムアウトしたファイル情報を記録するための辞書
                timeout_files = {}

                # 並列処理の実行
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers
                ) as executor:
                    # 各ファイルを並列処理
                    futures_dict = {}
                    file_info_dict = {}  # ファイル情報を保持する辞書
                    for file_info in files_to_process:
                        future = executor.submit(process_with_lock, file_info)
                        futures_dict[future] = file_info["file_path"]
                        file_info_dict[file_info["file_path"]] = (
                            file_info  # ファイル情報を保存
                        )
                        print(f"処理開始: {file_info['file_path']}")

                    # 処理中のファイル数を追跡
                    completed = 0
                    total = len(futures_dict)

                    # タイムアウト時間（秒）
                    timeout = 60

                    # 結果を集計
                    for future in concurrent.futures.as_completed(
                        futures_dict.keys(), timeout=timeout
                    ):
                        file_path = futures_dict[future]
                        try:
                            result = future.result(timeout=10)  # 個別のタイムアウト
                            completed += 1
                            print(f"処理完了 ({completed}/{total}): {file_path}")

                            if result["success"]:
                                stats["newly_processed"] += 1
                                print(f"  成功: {file_path}")
                            else:
                                stats["failed"] += 1
                                print(f"  失敗: {file_path}")
                        except concurrent.futures.TimeoutError:
                            completed += 1
                            print(
                                f"処理タイムアウト ({completed}/{total}): {file_path}"
                            )
                            stats["timeout"] += 1  # タイムアウトとしてカウント

                            # タイムアウトしたファイルの情報を記録
                            if file_path in file_info_dict:
                                timeout_files[file_path] = file_info_dict[file_path]
                        except Exception as e:
                            completed += 1
                            print(f"処理エラー ({completed}/{total}): {file_path}")
                            print(f"  エラー内容: {str(e)}")
                            stats["failed"] += 1

                    # 未完了のタスクをチェック
                    remaining = [
                        path
                        for future, path in futures_dict.items()
                        if not future.done()
                    ]
                    if remaining:
                        print(
                            f"警告: {len(remaining)}件のファイルが処理完了しませんでした:"
                        )
                        for path in remaining:
                            print(f"  - {path}")
                            stats["timeout"] += 1  # タイムアウトとしてカウント

                            # タイムアウトしたファイルの情報を記録
                            if path in file_info_dict:
                                timeout_files[path] = file_info_dict[path]

                    # タイムアウトしたファイルをデータベースから削除（処理済みマークを解除）
                    if timeout_files:
                        print(
                            f"タイムアウトした {len(timeout_files)} 件のファイルを処理済みマークから解除します"
                        )
                        for file_info in timeout_files.values():
                            # ファイルが処理済みとしてマークされている場合は削除
                            self.db_manager.unmark_file_as_processed(
                                file_info["file_path"], file_info["source_zip_str"]
                            )

                    # すべてのタスクが完了したことを確認
                    print(f"すべてのファイル処理が完了しました: {completed}/{total}")

        finally:
            # 一時ディレクトリを削除
            try:
                shutil.rmtree(temp_dir)
            except Exception as e:
                print(f"一時ディレクトリの削除中にエラー: {str(e)}")

        return stats

    def process_folder(self, folder_path=None, pattern=None, process_all=False):
        """
        フォルダ内のCSVファイルを処理する

        Parameters:
        folder_path (str or Path, optional): 検索対象のフォルダパス
        pattern (str, optional): 正規表現パターン
        process_all (bool): 処理済みファイルも再処理するかどうか

        Returns:
        dict: 処理結果の統計情報
        """
        # デフォルト値の設定
        folder_path = folder_path or config.get("folder")
        pattern = pattern or config.get("pattern")

        # ファイル検索
        print(f"フォルダ {folder_path} から条件に合うCSVファイルを検索中...")
        csv_files = self.find_csv_files(folder_path, pattern)
        print(f"{len(csv_files)}件のファイルが見つかりました")

        # ファイル処理
        print("CSVファイルの処理を開始します...")
        stats = self.process_csv_files(csv_files, process_all)

        return stats
</file>

<file path="README.md">
CSVセンサーデータ変換プログラムの実装内容についてまとめます。

# CSVセンサーデータ変換プログラム実装概要

このプログラムは、時系列のセンサーデータを含むCSVファイルをDuckDBデータベースに変換するツールです。

## フォルダ構成

```
csv_to_db/                      # プロジェクトルートディレクトリ
│
├── src/                        # ソースコード
│   ├── __init__.py             # Pythonパッケージとして認識させるためのファイル
│   ├── main.py                 # メインエントリーポイント
│   │
│   ├── config/                 # 設定関連
│   │   ├── __init__.py
│   │   └── config.py           # 設定管理クラス
│   │
│   ├── db/                     # データベース関連
│   │   ├── __init__.py
│   │   └── db_utils.py         # データベース操作クラス
│   │
│   ├── file/                   # ファイル操作関連
│   │   ├── __init__.py
│   │   ├── file_utils.py       # ファイル検索・ハッシュ計算
│   │   ├── zip_handler.py      # ZIPファイル処理
│   │   └── file_processor.py   # ファイル処理統合クラス
│   │
│   └── processor/              # データ処理関連
│       ├── __init__.py
│       └── csv_processor.py    # CSV処理クラス
│
├── tests/                      # テスト
│   ├── __init__.py
│   ├── test_config_values.py   # 設定値確認テスト
│   ├── test_env.py             # 環境変数読み込みテスト
│   └── test_main.py            # 機能テスト
│
├── data/                       # サンプルデータやデフォルトの入力ディレクトリ
│
├── main.py                     # プロジェクトルートからの実行用エントリーポイント
├── .env                        # 環境変数設定ファイル
├── .gitignore                  # Gitの除外設定
├── .python-version             # Pythonバージョン指定
├── pyproject.toml              # プロジェクト設定
├── uv.lock                     # 依存関係ロックファイル
└── README.md                   # プロジェクト説明
```

## 主要機能

1. **ファイル抽出機能**
   - 指定フォルダ内のCSVファイルを正規表現パターンで検索
   - ZIPファイル内のCSVも自動的に抽出して処理
   - `find_csv_files`関数と`extract_from_zip`関数で実装

2. **データベース管理機能**
   - DuckDBを使用してファイル処理履歴を管理
   - ファイルパスとハッシュ値で重複処理を防止
   - `setup_database`、`get_file_hash`、`is_file_processed_by_path`、`is_file_processed_by_hash`、`mark_file_as_processed`関数で実装

3. **処理実行機能**
   - CSVファイルを個別に処理する`process_csv_file`関数
   - ファイルリストを順次処理する`process_csv_files`関数
   - 処理結果の統計情報を収集

4. **その他の特徴**
   - Shift-JISエンコーディング対応（READMEに記載）
   - 特殊ヘッダー形式の処理（センサーID、センサー名、単位の行）
   - 縦持ちデータ形式への変換処理
   - コマンドラインインターフェース

## 技術スタック

- Python 3.11
- DuckDB: 処理履歴管理用のデータベース
- zipfile: ZIP圧縮ファイルの処理
- hashlib: ファイルハッシュ計算でコンテンツベースの重複チェック
- argparse: コマンドラインインターフェース実装

## 環境要件

- Python 3.11以上
- 依存ライブラリ: duckdb, pandas, polars, pyarrow

## 使用方法

プロジェクトのルートディレクトリから以下のコマンドで実行できます：

```bash
python main.py [オプション]
```

### 主なオプション

- `--folder`: 検索対象のフォルダパス（デフォルト: data）
- `--pattern`: ファイル名フィルタリングのための正規表現パターン（デフォルト: (Cond|User|test)）
- `--db`: 処理記録用データベースファイルのパス（デフォルト: processed_files.duckdb）
- `--process-all`: 処理済みファイルも再処理する場合に指定
- `--factory`: 工場名（デフォルト: AAA）
- `--machine-id`: 号機ID（デフォルト: No.1）
- `--data-label`: データラベル名（デフォルト: ２０２４年点検）

## 現状と今後の課題

- 実際のCSV処理ロジック（`process_csv_file`関数）は実装中
- センサーデータの縦持ち変換処理の実装完了が必要
</file>

<file path="pyproject.toml">
[project]
name = "csv-to-db"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "duckdb>=1.2.1",
    "numpy>=2.2.4",
    "pandas>=2.2.3",
    "polars>=1.26.0",
    "pyarrow>=19.0.1",
    "pytest>=8.3.5",
    "python-dotenv>=1.1.0",
]
</file>

<file path="main.py">
"""
CSVファイル処理ツール - エントリーポイント

このスクリプトは、src/main.pyのメイン関数を呼び出すためのエントリーポイントです。
"""

from src.main import main

if __name__ == "__main__":
    main()
</file>

</files>
