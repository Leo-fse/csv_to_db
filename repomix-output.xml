This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
data/
  test copy.csv
  test.csv
  test2 copy.csv
  test2.csv
.gitignore
.python-version
convert_to_sjis.py
main.py
pyproject.toml
README.md
test_main.py
test.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="data/test copy.csv">
, 1000, 1001, 1000, 1002
, param_A, param_B, param_A, param_C
, kg, mm, kg, cm
2024/1/1 00:00:05,1,2,"a",4,
2024/1/1 00:00:06,1,2,"a",4,
2024/1/1 00:00:07,1,2,"a",4,
2024/1/1 00:00:08,1,2,"a",4,
2024/1/1 00:00:09,1,2,"a",4,
</file>

<file path="data/test.csv">
, 1000, 1001, 1000, 1002
, param_A, param_B, param_A, param_C
, kg, mm, kg, cm
2024/1/1 00:00:00,1,2,"a",4,
2024/1/1 00:00:01,1,2,"a",4,
2024/1/1 00:00:02,1,2,"a",4,
2024/1/1 00:00:03,1,2,"a",4,
2024/1/1 00:00:04,1,2,"a",4,
</file>

<file path="data/test2 copy.csv">
, 1003, 1004, 1002, 1005
, param_D, param_E, param_C, param_F
, kg, mm, cm, cm
2024/1/1 00:00:05,1,2,"a",4,
2024/1/1 00:00:06,1,2,"a",4,
2024/1/1 00:00:07,1,2,"a",4,
2024/1/1 00:00:08,1,2,"a",4,
2024/1/1 00:00:09,1,2,"a",4,
</file>

<file path="data/test2.csv">
, 1003, 1004, 1002, 1005
, param_D, param_E, param_C, param_F
, kg, mm, cm, cm
2024/1/1 00:00:00,1,2,"a",4,
2024/1/1 00:00:01,1,2,"a",4,
2024/1/1 00:00:02,1,2,"a",4,
2024/1/1 00:00:03,1,2,"a",4,
2024/1/1 00:00:04,1,2,"a",4,
</file>

<file path=".gitignore">
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
</file>

<file path=".python-version">
3.11
</file>

<file path="convert_to_sjis.py">
"""
CSVファイルをUTF-8からShift-JISエンコーディングに変換するスクリプト
"""

import os

def convert_encoding(input_file, output_file):
    """
    ファイルをUTF-8からShift-JISに変換する
    
    Args:
        input_file: 入力ファイルパス（UTF-8）
        output_file: 出力ファイルパス（Shift-JIS）
    """
    # UTF-8でファイルを読み込む
    with open(input_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Shift-JISで書き込む
    with open(output_file, 'w', encoding='shift-jis') as f:
        f.write(content)
    
    print(f"ファイルを変換しました: {input_file} → {output_file}")

if __name__ == "__main__":
    # 入出力ファイルパス
    input_file = "data/test_jp_Cond.csv"
    output_file = "data/test_jp_Cond_sjis.csv"
    
    convert_encoding(input_file, output_file)
</file>

<file path="test_main.py">
"""
main.pyのユニットテスト

特定の機能をテストするための単体テスト
"""

import unittest
import os
import tempfile
import shutil
from io import StringIO
import zipfile
from unittest.mock import patch, MagicMock
import polars as pl
import duckdb

# テスト対象のモジュールをインポート
import main


class TestCSVProcessing(unittest.TestCase):
    """CSVファイル処理のテスト"""

    def setUp(self):
        """テスト前の準備"""
        # テスト用の一時ディレクトリを作成
        self.test_dir = tempfile.mkdtemp()
        
        # テスト用CSVファイルの内容
        self.test_csv_content = """
, 1000, 1001, 1000, 1002
, param_A, param_B, param_A, param_C
, kg, mm, kg, cm
2024/1/1 00:00:00,1,2,"a",4,
2024/1/1 00:00:01,1,2,"a",4,
2024/1/1 00:00:02,1,2,"a",4,
""".strip()
        
        # テスト用CSVファイルを作成
        self.normal_csv_path = os.path.join(self.test_dir, "test_Cond.csv")
        with open(self.normal_csv_path, "w", encoding="shift-jis") as f:
            f.write(self.test_csv_content)
        
        # テスト用ZIPファイルを作成
        self.zip_path = os.path.join(self.test_dir, "archive.zip")
        with zipfile.ZipFile(self.zip_path, "w") as zip_file:
            zip_file.writestr("test_User.csv", self.test_csv_content)
        
        # テスト用データベースファイル
        self.db_path = os.path.join(self.test_dir, "test.duckdb")

    def tearDown(self):
        """テスト後のクリーンアップ"""
        # テスト用一時ディレクトリを削除
        shutil.rmtree(self.test_dir)

    def test_get_target_files(self):
        """ファイルリスト取得機能のテスト"""
        # パターンにマッチするファイルのみ取得されるかテスト
        target_files = main.get_target_files(self.test_dir, "Cond")
        self.assertEqual(len(target_files), 1)
        self.assertEqual(target_files[0]["type"], "normal")
        self.assertEqual(target_files[0]["full_path"], self.normal_csv_path)
        
        # ZIP内のファイルも取得されるかテスト
        target_files = main.get_target_files(self.test_dir, "User")
        self.assertEqual(len(target_files), 1)
        self.assertEqual(target_files[0]["type"], "zip")
        self.assertEqual(target_files[0]["zip_path"], self.zip_path)
        self.assertEqual(target_files[0]["inner_path"], "test_User.csv")
        
        # 複数パターンのテスト
        target_files = main.get_target_files(self.test_dir, "Cond|User")
        self.assertEqual(len(target_files), 2)

    def test_read_csv_headers(self):
        """CSVヘッダー読み込み機能のテスト"""
        # 本文の最初の3行を取得
        content_lines = self.test_csv_content.splitlines()[:3]
        
        # ヘッダー読み込み
        sensor_ids, sensor_names, sensor_units = main.read_csv_headers(content_lines)
        
        # 想定される結果を検証（空白が除去された結果を期待）
        self.assertEqual(sensor_ids, ["1000", "1001", "1000", "1002"])
        self.assertEqual(sensor_names, ["param_A", "param_B", "param_A", "param_C"])
        self.assertEqual(sensor_units, ["kg", "mm", "kg", "cm"])

    def test_convert_to_vertical_df(self):
        """縦持ちデータ変換機能のテスト"""
        # テスト用データフレーム
        test_df = pl.DataFrame({
            "timestamp": ["2024/1/1 00:00:00", "2024/1/1 00:00:01"],
            "value_0": [1, 1],
            "value_1": [2, 2],
            "value_2": ["a", "a"],
            "value_3": [4, 4]
        })
        
        # センサー情報
        sensor_ids = ["1000", "1001", "1000", "1002"]
        sensor_names = ["param_A", "param_B", "param_A", "param_C"]
        sensor_units = ["kg", "mm", "kg", "cm"]
        
        # 縦持ちデータ変換
        result_df = main.convert_to_vertical_df(
            test_df, sensor_ids, sensor_names, sensor_units, "TestPlant", "TestMachine"
        )
        
        # 結果を検証
        self.assertIsNotNone(result_df)
        self.assertEqual(len(result_df), 8)  # 2行 × 4センサー = 8行
        
        # 特定のセンサー値を確認（文字列型に変換されていることに注意）
        first_row = result_df.filter(
            (pl.col("sensor_id") == "1000") & (pl.col("sensor_name") == "param_A")
        ).row(0)
        self.assertEqual(first_row[0], "TestPlant")  # plant_name
        self.assertEqual(first_row[1], "TestMachine")  # machine_no
        self.assertEqual(first_row[2], "2024/1/1 00:00:00")  # timestamp
        self.assertEqual(first_row[6], "1")  # value - 文字列型に変換されていることを確認

    def test_database_operations(self):
        """データベース操作機能のテスト"""
        try:
            # テスト用データベース接続
            conn = main.init_database(self.db_path)
            
            # ファイル情報
            file_info = {"full_path": self.normal_csv_path}
            
            # 初期状態では処理済みでないことを確認
            self.assertFalse(main.is_file_processed(conn, file_info))
            
            # 処理済みとしてマーク
            main.mark_file_as_processed(conn, file_info)
            
            # 処理済みになったことを確認
            self.assertTrue(main.is_file_processed(conn, file_info))
            
            # データベース接続を閉じる
            conn.close()
        except Exception as e:
            self.fail(f"データベース操作テストが失敗しました: {e}")


if __name__ == "__main__":
    unittest.main()
</file>

<file path="README.md">
# CSVセンサーデータ変換プログラム

センサーデータを含むCSVファイルをDuckDBデータベースに変換するプログラムです。時系列のセンサーデータを効率的に管理・分析するためのツールです。

## 特徴

このプログラムは以下の特徴を持っています：

1. **Shift-JISエンコーディング対応**：
   - 日本語を含むShift-JISエンコードのCSVファイルを正しく読み込み

2. **特殊ヘッダー形式の処理**：
   - 1行目: センサーID
   - 2行目: センサー名
   - 3行目: 単位
   - 4行目以降: タイムスタンプとデータ

3. **データのクリーニング**：
   - 末尾の余分なカンマの処理
   - 不要列のフィルタリング（センサー名・単位が"-"のもの）

4. **縦持ちデータ形式への変換**：
   - 横持ちCSVデータを「PlantName, MachineNo, Time, sensor_id, sensor_name, sensor_unit, value」の縦持ちデータに変換

5. **ZIPファイルサポート**：
   - ZIP圧縮されたCSVファイルも直接読み込み可能

6. **ファイル名フィルタリング**：
   - パターンマッチによる対象ファイルの選別

7. **重複処理の防止**：
   - 処理済みファイルのスキップ機能

## 要件

- Python 3.11以上
- 以下のライブラリ：
  - polars >= 1.26.0
  - duckdb >= 1.2.1
  - pyarrow >= 19.0.1

## インストール

```bash
# 仮想環境を作成
python -m venv .venv
source .venv/bin/activate  # Windowsの場合: .venv\Scripts\activate

# 依存パッケージのインストール
pip install -e .
```

## 使用方法

```bash
# 基本的な使用方法
python main.py --plant_name [プラント名] --machine_no [機器番号] --file_pattern [パターン]

# 例：プラント名「Plant1」、機器番号「Machine1」、ファイル名に「Cond」または「User」を含むファイルを処理
python main.py --plant_name Plant1 --machine_no Machine1 --file_pattern "Cond|User"

# ディレクトリ指定（デフォルトは「data」）
python main.py --csv_path /path/to/csv/files --plant_name Plant1 --machine_no Machine1
```

## コマンドラインオプション

| オプション       | 説明                                           | デフォルト値   |
|----------------|------------------------------------------------|--------------|
| --csv_path     | CSVファイルを含むディレクトリパス                    | data         |
| --plant_name   | プラント名（必須）                               | -            |
| --machine_no   | 機器番号（必須）                                 | -            |
| --file_pattern | 処理対象ファイル名のパターン（正規表現）               | Cond\|User   |

## データベース構造

データベースには以下の2つのテーブルが作成されます：

1. **sensor_data**：センサーデータの縦持ち形式のテーブル
   - id: 自動増分主キー
   - plant_name: プラント名
   - machine_no: 機器番号
   - time: タイムスタンプ
   - sensor_id: センサーID
   - sensor_name: センサー名
   - sensor_unit: 単位
   - value: センサー値（文字列として格納）

2. **processed_files**：処理済みファイル管理テーブル
   - file_path: ファイルパス（主キー）
   - processed_at: 処理日時

## 技術的特長

- **高速データ処理**：Polarsを使用した効率的なデータ処理
- **堅牢なエラー処理**：各処理段階でのエラーハンドリング
- **柔軟なデータ型対応**：数値と文字列両方に対応する値格納
- **メモリ効率**：大規模ファイルにも対応するストリーミング処理

## ファイル構成

- `main.py`: メインプログラム
- `test_main.py`: ユニットテスト
- `pyproject.toml`: プロジェクト設定
- `data/`: サンプルデータディレクトリ

## ライセンス

このプロジェクトはMITライセンスのもとで公開されています。
</file>

<file path="test.py">
from pathlib import Path
import re
import zipfile
import tempfile
import os
import duckdb
import hashlib
import datetime
import shutil

# ====== ファイル抽出部分 ======

def find_csv_files(folder_path, pattern):
    """
    フォルダ内およびZIPファイル内から正規表現パターンに一致するCSVファイルを抽出する
    
    Parameters:
    folder_path (str or Path): 検索対象のフォルダパス
    pattern (str): 正規表現パターン
    
    Returns:
    list: [{'path': ファイルパス, 'source_zip': ZIPファイルパス（ない場合はNone）}]
    """
    found_files = []
    
    # Pathオブジェクトへ変換
    folder = Path(folder_path)
    
    # コンパイル済み正規表現パターン
    regex = re.compile(pattern)
    
    # 通常のCSVファイルを検索
    for file in folder.rglob("*.csv"):
        if regex.search(file.name):
            found_files.append({
                'path': file,
                'source_zip': None
            })
    
    # ZIPファイルを検索して中身を確認
    for zip_file in folder.rglob("*.zip"):
        try:
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                # ZIPファイル内のファイル一覧を取得
                zip_contents = zip_ref.namelist()
                
                # CSVファイルかつ条件に合うものを抽出
                for file_in_zip in zip_contents:
                    if file_in_zip.endswith('.csv') and regex.search(Path(file_in_zip).name):
                        found_files.append({
                            'path': file_in_zip,
                            'source_zip': zip_file
                        })
        except zipfile.BadZipFile:
            print(f"警告: {zip_file}は有効なZIPファイルではありません。")
    
    return found_files

def extract_from_zip(zip_path, file_path, output_dir):
    """
    ZIPファイルから特定のファイルを抽出する（改良版）
    
    Parameters:
    zip_path (str or Path): ZIPファイルのパス
    file_path (str): 抽出するファイルのZIP内パス
    output_dir (str or Path): 出力先ディレクトリ
    
    Returns:
    Path: 抽出されたファイルのパス
    """
    # 出力ディレクトリの確認と作成
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ZIPファイルを開いて処理
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # ZIPファイル内のファイルパスを正規化
        normalized_path = file_path.replace('\\', '/')
        
        # ファイル名のみを取得
        file_name = Path(normalized_path).name
        
        # 出力先のフルパス
        output_path = output_dir / file_name
        
        # ファイルを抽出
        try:
            # まずそのままのパスで試す
            zip_ref.extract(normalized_path, output_dir)
            # 階層構造があればそのファイルへのフルパスを返す
            if '/' in normalized_path:
                return output_dir / normalized_path
            return output_path
        except KeyError:
            # 正確なパスでなければ、ファイル名でマッチするものを探す
            for zip_info in zip_ref.infolist():
                zip_file_path = zip_info.filename.replace('\\', '/')
                if zip_file_path.endswith('/' + file_name) or zip_file_path == file_name:
                    # 見つかったファイルを抽出
                    zip_ref.extract(zip_info, output_dir)
                    # 抽出されたファイルのパスを返す
                    if '/' in zip_info.filename:
                        return output_dir / zip_info.filename
                    return output_dir / file_name
            
            # ファイルが見つからない場合はエラー
            raise FileNotFoundError(f"ZIPファイル内に {file_path} または {file_name} が見つかりません。")

# ====== データベース管理部分 ======

def setup_database(db_path):
    """DuckDBデータベースを初期化する"""
    conn = duckdb.connect(str(db_path))
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            file_path VARCHAR NOT NULL,,
            file_hash VARCHAR NOT NULL,,
            source_zip VARCHAR,
            processed_date TIMESTAMP,
            PRIMARY KEY (file_path, source_zip)
        )
    """)
    return conn

def get_file_hash(file_path):
    """ファイルのSHA256ハッシュを計算する"""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        # ファイルを小さなチャンクで読み込んでハッシュ計算
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def is_file_processed(conn, file_path, source_zip=None):
    """ファイルが既に処理済みかどうかを確認する"""
    source_zip_value = "" if source_zip is None else str(source_zip)
    result = conn.execute(
            "SELECT COUNT(*) FROM processed_files WHERE file_path = ? AND source_zip = ?",
            [str(file_path), source_zip]
        ).fetchone()

    
    return result[0] > 0

def mark_file_as_processed(conn, file_path, file_hash, source_zip=None):
    """ファイルを処理済みとしてデータベースに記録する"""
    now = datetime.datetime.now()
    source_zip_value = "" if source_zip is None else str(source_zip)
    conn.execute(
        "INSERT INTO processed_files (file_path, file_hash, source_zip, processed_date) VALUES (?, ?, ?, ?)",
        [str(file_path), file_hash, source_zip, now]
    )

# ====== 処理実行部分 ======

def process_csv_file(file_path):
    """
    CSVファイルを処理する関数（実際の処理はここに実装）
    
    Parameters:
    file_path (Path): 処理するCSVファイルのパス
    
    Returns:
    bool: 処理が成功したかどうか
    """
    print(f"処理中: {file_path}")
    
    # ここに実際のCSV処理ロジックを実装する
    # 例: pandas でCSVを読み込んで何らかの処理を行う
    # import pandas as pd
    # df = pd.read_csv(file_path)
    # ... 処理 ...
    
    # 処理が成功したことを示す（実際の実装に応じて変更）
    return True

def process_csv_files(csv_files, db_path, process_all=False):
    """
    CSVファイルのリストを処理する
    
    Parameters:
    csv_files (list): 処理するCSVファイルのリスト
    db_path (str or Path): DuckDBデータベースのパス
    process_all (bool): 処理済みファイルも再処理するかどうか
    
    Returns:
    dict: 処理結果の統計情報
    """
    # 結果統計
    stats = {
        "total_found": len(csv_files),
        "already_processed": 0,
        "newly_processed": 0,
        "failed": 0
    }
    
    # データベース接続
    conn = setup_database(db_path)
    
    # 一時ディレクトリを作成
    temp_dir = Path(tempfile.mkdtemp())
    
    try:
        for file_info in csv_files:
            file_path = file_info['path']
            source_zip = file_info['source_zip']
            
            # 既に処理済みかチェック
            if not process_all and is_file_processed(conn, file_path, str(source_zip) if source_zip else None):
                stats["already_processed"] += 1
                print(f"スキップ (既処理): {file_path}" + (f" (in {source_zip})" if source_zip else ""))
                continue
            
            try:
                # ZIPファイル内のファイルなら抽出
                if source_zip:
                    # 一時ファイルにZIPから抽出
                    actual_file_path = extract_from_zip(source_zip, file_path, temp_dir)
                else:
                    actual_file_path = file_path
                
                # ファイルを処理
                if process_csv_file(actual_file_path):
                    # ハッシュを計算して処理済みとマーク
                    file_hash = get_file_hash(actual_file_path)
                    mark_file_as_processed(
                        conn, 
                        file_path, 
                        file_hash, 
                        str(source_zip) if source_zip else None
                    )
                    stats["newly_processed"] += 1
                else:
                    stats["failed"] += 1
            except Exception as e:
                print(f"エラー処理中 {file_path}" + (f" (in {source_zip})" if source_zip else "") + f": {str(e)}")
                stats["failed"] += 1
    
    finally:
        # 一時ディレクトリを削除
        shutil.rmtree(temp_dir)
        
        # データベース接続をコミットして閉じる
        conn.commit()
        conn.close()
    
    return stats

# ====== メイン実行部分 ======

def main(folder_path, pattern, db_path, process_all=False):
    """
    メイン実行関数
    
    Parameters:
    folder_path (str or Path): 検索対象のフォルダパス
    pattern (str): 正規表現パターン
    db_path (str or Path): DuckDBデータベースのパス
    process_all (bool): 処理済みファイルも再処理するかどうか
    """
    # ファイル検索（抽出部分）
    print(f"フォルダ {folder_path} から条件に合うCSVファイルを検索中...")
    csv_files = find_csv_files(folder_path, pattern)
    print(f"{len(csv_files)}件のファイルが見つかりました")
    
    # ファイル処理（処理部分）
    print("CSVファイルの処理を開始します...")
    stats = process_csv_files(csv_files, db_path, process_all)
    
    # 結果の表示
    print("\n---- 処理結果 ----")
    print(f"見つかったファイル数: {stats['total_found']}")
    print(f"既に処理済み: {stats['already_processed']}")
    print(f"新たに処理: {stats['newly_processed']}")
    print(f"処理失敗: {stats['failed']}")

# 使用例
if __name__ == "__main__":
    folder_path = "検索したいフォルダのパス"  # ここに実際のフォルダパスを入力
    db_path = "processed_files.duckdb"        # DuckDBデータベースのパス
    
    # "Cond"または"User"を含むファイル名の正規表現パターン
    pattern = r"(Cond|User)"
    
    # 処理実行
    main(folder_path, pattern, db_path, process_all=False)
</file>

<file path="main.py">
"""
センサーデータCSVファイルを読み込み、DuckDBデータベースに変換するプログラム

特徴:
- Shift-JISエンコードのCSVファイル対応
- 3行ヘッダー形式（センサーID、センサー名、単位）の処理
- 末尾カンマの処理
- ZIPファイル内のCSVファイル対応
- ファイル名パターンによるフィルタリング
- 処理済みファイルのスキップ
- 縦持ちデータ形式への変換

使用方法:
python main.py --csv_path data --plant_name Plant1 --machine_no Machine1 --file_pattern "Cond|User"
"""

import os
import re
import zipfile
import io
import argparse
from datetime import datetime
import polars as pl
import duckdb


def parse_args():
    """コマンドライン引数を解析する"""
    parser = argparse.ArgumentParser(description="CSVセンサーデータをDuckDBに変換")
    parser.add_argument('--csv_path', type=str, default='data',
                        help='CSVファイルのあるディレクトリ')
    parser.add_argument('--plant_name', type=str, required=True,
                        help='プラント名')
    parser.add_argument('--machine_no', type=str, required=True,
                        help='機器番号')
    parser.add_argument('--file_pattern', type=str, default='Cond|User',
                        help='ファイル名フィルタリングのための正規表現パターン（デフォルト: "Cond|User"）')
    return parser.parse_args()


def get_target_files(base_dir, pattern):
    """
    通常のCSVファイルとZIPファイル内のCSVファイルをリストアップ
    
    Args:
        base_dir: 基本ディレクトリ
        pattern: ファイル名のフィルタリングパターン（正規表現）
    
    Returns:
        処理対象ファイルのリスト（パスとZIP内パスの組み合わせ）
    """
    target_files = []
    pattern_regex = re.compile(pattern)
    
    # ディレクトリ内の全ファイルを走査
    for root, _, files in os.walk(base_dir):
        for filename in files:
            file_path = os.path.join(root, filename)
            
            # ZIPファイルの場合
            if filename.lower().endswith('.zip'):
                try:
                    with zipfile.ZipFile(file_path, 'r') as zip_ref:
                        for zip_info in zip_ref.infolist():
                            # ZIPファイル内のCSVファイルをチェック
                            if zip_info.filename.lower().endswith('.csv'):
                                # パターンに一致するかチェック
                                if pattern_regex.search(os.path.basename(zip_info.filename)):
                                    target_files.append({
                                        'type': 'zip',
                                        'zip_path': file_path,
                                        'inner_path': zip_info.filename,
                                        'full_path': f"{file_path}:{zip_info.filename}"
                                    })
                except Exception as e:
                    print(f"警告: ZIPファイルの読み込みエラー {file_path}: {e}")
            
            # 通常のCSVファイルの場合
            elif filename.lower().endswith('.csv'):
                # パターンに一致するかチェック
                if pattern_regex.search(filename):
                    target_files.append({
                        'type': 'normal',
                        'full_path': file_path
                    })
    
    return target_files


def read_csv_headers(content_lines):
    """
    CSVファイルの最初の3行からヘッダー情報を抽出
    
    Args:
        content_lines: CSVファイルの行のリスト
    
    Returns:
        (sensor_ids, sensor_names, sensor_units)のタプル
    """
    if len(content_lines) < 3:
        raise ValueError("CSVファイルは少なくとも3行のヘッダーが必要です")
    
    # 各行を解析（最初の列と末尾の余分な列を除外）
    sensor_ids = content_lines[0].strip().split(',')
    sensor_names = content_lines[1].strip().split(',')
    sensor_units = content_lines[2].strip().split(',')
    
    # 最初の列（時間列のヘッダー）を除外
    sensor_ids = sensor_ids[1:]
    sensor_names = sensor_names[1:]
    sensor_units = sensor_units[1:]
    
    # 末尾の余分なカンマを処理（空の要素を削除）
    if sensor_ids and sensor_ids[-1] == '':
        sensor_ids = sensor_ids[:-1]
    if sensor_names and sensor_names[-1] == '':
        sensor_names = sensor_names[:-1]
    if sensor_units and sensor_units[-1] == '':
        sensor_units = sensor_units[:-1]
    
    # 各値の前後の空白を削除
    sensor_ids = [sid.strip() for sid in sensor_ids]
    sensor_names = [name.strip() for name in sensor_names]
    sensor_units = [unit.strip() for unit in sensor_units]
    
    return sensor_ids, sensor_names, sensor_units


def read_csv_file(file_info):
    """
    通常のCSVファイルまたはZIP内のCSVファイルを読み込む
    
    Args:
        file_info: ファイル情報の辞書
    
    Returns:
        (sensor_ids, sensor_names, sensor_units, data_df)のタプル
    """
    try:
        if file_info['type'] == 'normal':
            # 通常のファイルの場合
            file_path = file_info['full_path']
            
            # ヘッダー3行を別々に読み込む
            with open(file_path, 'r', encoding='shift-jis') as f:
                content_lines = f.readlines()
                sensor_ids, sensor_names, sensor_units = read_csv_headers(content_lines)
            
            # データ部分（4行目以降）を読み込む
            data_df = pl.read_csv(
                file_path,
                encoding="shift-jis",
                skip_rows=3,
                has_header=False,
                truncate_ragged_lines=True
            )
        
        else:  # 'zip'の場合
            # ZIPファイル内のCSVを読み込む
            with zipfile.ZipFile(file_info['zip_path'], 'r') as zip_ref:
                with zip_ref.open(file_info['inner_path']) as csv_file:
                    # ZIPからバイナリとして読み込んでデコード
                    content = csv_file.read().decode('shift-jis')
                    content_lines = content.splitlines()
                    
                    # ヘッダー3行を解析
                    sensor_ids, sensor_names, sensor_units = read_csv_headers(content_lines)
                    
                    # データ部分をPolarsで読み込む
                    data_content = '\n'.join(content_lines[3:])
                    data_df = pl.read_csv(
                        io.StringIO(data_content),
                        has_header=False,
                        truncate_ragged_lines=True
                    )
        
        # 余分な列を削除（末尾カンマによる）
        if data_df.shape[1] > len(sensor_ids) + 1:
            data_df = data_df.drop(data_df.columns[-1])
        
        # 列名を設定（1列目は時間、残りはセンサー値）
        data_df.columns = ["timestamp"] + [f"value_{i}" for i in range(len(sensor_ids))]
        
        return sensor_ids, sensor_names, sensor_units, data_df
    
    except Exception as e:
        raise ValueError(f"CSVファイルの読み込みエラー: {e}")


def is_file_processed(conn, file_info):
    """
    ファイルが既に処理済みかチェック
    
    Args:
        conn: DuckDB接続
        file_info: ファイル情報の辞書
    
    Returns:
        処理済みならTrue、そうでなければFalse
    """
    # ZIPファイルの場合はZIPパスと内部パスの組み合わせをキーとする
    file_path = file_info['full_path']
    
    result = conn.execute("""
        SELECT 1 FROM processed_files
        WHERE file_path = ?
    """, [file_path]).fetchone()
    
    return result is not None


def mark_file_as_processed(conn, file_info):
    """
    ファイルを処理済みとマーク
    
    Args:
        conn: DuckDB接続
        file_info: ファイル情報の辞書
    """
    file_path = file_info['full_path']
    
    conn.execute("""
        INSERT INTO processed_files (file_path, processed_at)
        VALUES (?, CURRENT_TIMESTAMP)
    """, [file_path])


def convert_to_vertical_df(data_df, sensor_ids, sensor_names, sensor_units, plant_name, machine_no):
    """
    センサーデータを縦持ちデータフレームに変換
    
    Args:
        data_df: センサーデータのデータフレーム
        sensor_ids: センサーID配列
        sensor_names: センサー名配列
        sensor_units: センサー単位配列
        plant_name: プラント名
        machine_no: 機器番号
    
    Returns:
        縦持ちデータフレーム
    """
    vertical_data = []
    
    for i in range(len(sensor_ids)):
        sensor_id = sensor_ids[i]
        sensor_name = sensor_names[i]
        sensor_unit = sensor_units[i]
        
        # 不要センサーのスキップ
        if sensor_name == "-" and sensor_unit == "-":
            continue
        
        # 値の列インデックス
        value_col = f"value_{i}"
        
        # 縦持ちデータ作成
        try:
            # すべての値を文字列として扱うことで型の互換性問題を回避
            sensor_df = data_df.select([
                pl.lit(plant_name).alias("plant_name"),
                pl.lit(machine_no).alias("machine_no"),
                pl.col("timestamp"),
                pl.lit(sensor_id).alias("sensor_id"),
                pl.lit(sensor_name).alias("sensor_name"),
                pl.lit(sensor_unit).alias("sensor_unit"),
                pl.col(value_col).cast(pl.Utf8).alias("value")  # すべての値を文字列に変換
            ])
            vertical_data.append(sensor_df)
        except Exception as e:
            print(f"警告: センサーID:{sensor_id}, 名前:{sensor_name}の処理中にエラーが発生しました: {e}")
    
    # 全センサーデータの結合
    if not vertical_data:
        return None
    
    return pl.concat(vertical_data)


def init_database(db_path):
    """
    データベースの初期化
    
    Args:
        db_path: データベースファイルのパス
    
    Returns:
        DuckDB接続オブジェクト
    """
    conn = duckdb.connect(db_path)
    
    # テーブル作成（初回のみ）
    conn.execute("""
        CREATE SEQUENCE IF NOT EXISTS sensor_id_seq;
        
        CREATE TABLE IF NOT EXISTS sensor_data (
            id INTEGER DEFAULT(nextval('sensor_id_seq')),
            plant_name VARCHAR,
            machine_no VARCHAR,
            time TIMESTAMP,
            sensor_id VARCHAR,
            sensor_name VARCHAR,
            sensor_unit VARCHAR,
            value VARCHAR,  -- 数値か文字列かに応じて柔軟に対応する文字列型
            PRIMARY KEY(id)
        )
    """)
    
    conn.execute("""
        CREATE TABLE IF NOT EXISTS processed_files (
            file_path VARCHAR PRIMARY KEY,
            processed_at TIMESTAMP
        )
    """)
    
    return conn


def main():
    """メイン処理"""
    # コマンドライン引数の解析
    args = parse_args()
    
    print(f"開始: {datetime.now()}")
    print(f"ディレクトリ: {args.csv_path}")
    print(f"プラント名: {args.plant_name}")
    print(f"機器番号: {args.machine_no}")
    print(f"ファイルパターン: {args.file_pattern}")
    
    # データベース接続
    conn = init_database("sensor_data.duckdb")
    
    # 処理対象ファイルのリストを取得
    target_files = get_target_files(args.csv_path, args.file_pattern)
    
    print(f"{len(target_files)}個のファイルが処理対象として見つかりました")
    
    # 処理済みファイル数と新規処理ファイル数のカウンタ
    processed_count = 0
    skipped_count = 0
    
    # 各ファイルを処理
    for file_info in target_files:
        file_path = file_info['full_path']
        
        # 既に処理済みかチェック
        if is_file_processed(conn, file_info):
            print(f"スキップ: {file_path} (既に処理済み)")
            skipped_count += 1
            continue
        
        print(f"処理中: {file_path}")
        
        try:
            # ファイル読み込み
            sensor_ids, sensor_names, sensor_units, data_df = read_csv_file(file_info)
            
            # 縦持ちデータに変換
            result_df = convert_to_vertical_df(
                data_df, sensor_ids, sensor_names, sensor_units, 
                args.plant_name, args.machine_no
            )
            
            if result_df is not None and len(result_df) > 0:
                # タイムスタンプの変換
                try:
                    result_df = result_df.with_columns(
                        pl.col("timestamp").str.to_datetime("%Y/%m/%d %H:%M:%S").alias("time")
                    ).drop("timestamp")
                except Exception as e:
                    print(f"  警告: タイムスタンプ変換エラー: {e}")
                    result_df = result_df.rename({"timestamp": "time"})
                
                # データの挿入（IDは自動生成させる）
                conn.execute("""
                    INSERT INTO sensor_data (plant_name, machine_no, time, sensor_id, sensor_name, sensor_unit, value)
                    SELECT plant_name, machine_no, time, sensor_id, sensor_name, sensor_unit, value 
                    FROM result_df
                """)
                
                row_count = len(result_df)
                print(f"  {row_count}行のデータを挿入しました")
                
                # 処理済みマーク
                mark_file_as_processed(conn, file_info)
                processed_count += 1
            else:
                print(f"  処理可能なセンサーデータが見つかりませんでした")
            
        except Exception as e:
            print(f"エラー: {file_path} の処理中にエラーが発生しました: {e}")
    
    # 接続を閉じる
    conn.close()
    
    print("\n処理サマリ:")
    print(f"- 処理対象ファイル数: {len(target_files)}")
    print(f"- 処理済みファイル数: {processed_count}")
    print(f"- スキップされたファイル数: {skipped_count}")
    print(f"- エラーファイル数: {len(target_files) - processed_count - skipped_count}")
    print(f"\n完了: {datetime.now()}")


if __name__ == "__main__":
    main()
</file>

<file path="pyproject.toml">
[project]
name = "csv-to-db"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "duckdb>=1.2.1",
    "pandas>=2.2.3",
    "polars>=1.26.0",
    "pyarrow>=19.0.1",
]
</file>

</files>
